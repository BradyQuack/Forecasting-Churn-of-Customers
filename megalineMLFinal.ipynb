{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing Machine Learning to Optimize Subscriber Plan\n",
    "## Overview and Project Setup\n",
    "Mobile carrier Megaline has discovered that many of its subscribers are still on legacy plans, even though newer plans offer enhanced features. The challenge lies in understanding subscriber behavior to effectively recommend one of the new plans, thereby increasing customer satisfaction and operational efficiency.\n",
    "\n",
    "### Data Exploration:\n",
    "(1) Load the dataset and perform an initial exploration to understand the distributions, central tendencies, and any anomalies in the data.\n",
    "\n",
    "(2) Verify data quality and check for missing values or outliers.\n",
    "### Data Splitting:\n",
    "(1) Split the data into training, validation, and test sets using a stratified approach to preserve the balance of the target classes.\n",
    "### Model Development and Hyperparameter Tuning:\n",
    "(1) Experiment with different models such as Logistic Regression, Decision Trees, and ensemble methods like Random Forest and Gradient Boosting.\n",
    "\n",
    "(2) Utilize grid search with cross-validation on the training set to optimize model hyperparameters.\n",
    "\n",
    "(3) Evaluate model performance on the validation set to ensure the accuracy threshold is met.\n",
    "### Final Model Evaluation:\n",
    "(1) Once an optimal model is identified, conduct a final evaluation on the test set to confirm its generalization performance.\n",
    "\n",
    "(2) Perform additional sanity checks to understand the model's behavior and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import set_config\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   calls     3214 non-null   float64\n",
      " 1   minutes   3214 non-null   float64\n",
      " 2   messages  3214 non-null   float64\n",
      " 3   mb_used   3214 non-null   float64\n",
      " 4   is_ultra  3214 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 125.7 KB\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "data = pd.read_csv('/datasets/users_behavior.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe Data:\n",
    "Each record represents one subscriber’s activity for a given month, providing a comprehensive view of how they interact with the mobile carrier’s services. The dataset contains no missing values, ensuring that each record is complete. All features are numerical, with the first four being floating-point numbers and the target variable as an integer. By capturing both voice and data usage metrics, the dataset allows for a nuanced analysis of subscriber behavior, which is essential for determining the most appropriate plan recommendation of Smart or Ultra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 1928 1928.3999999999999\n",
      "Validation Size: 643 642.8000000000001\n",
      "Test Size: 643 642.8000000000001\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "features = data.drop(['is_ultra'], axis=1)\n",
    "target = data['is_ultra']\n",
    "\n",
    "# 80% training / val and 20% test\n",
    "features_train_val, features_test, target_train_val, target_test = train_test_split(\n",
    "    features, \n",
    "    target,\n",
    "    test_size=0.20,\n",
    "    random_state=12345\n",
    ")\n",
    "\n",
    "# 60% Training and 20% val\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features_train_val,\n",
    "    target_train_val, \n",
    "    test_size=0.25, \n",
    "    random_state=12345\n",
    ")\n",
    "\n",
    "print(\"Training Size:\", features_train.shape[0], 3214 * 0.6)\n",
    "print(\"Validation Size:\", features_valid.shape[0], 3214 * 0.2)\n",
    "print(\"Test Size:\", features_test.shape[0], 3214 * 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "The dataset, consisting of 3214 records, was split into three subsets to ensure that the model can be properly trained, validated, and tested. First, 80% of the data was separated into a combined training and validation set while reserving the remaining 20% as a test set. Then, the training and validation set was further divided into 75% for training and 25% for validation. This results in an overall split of 60% training data (approximately 1928 records), 20% validation data (around 643 records), and 20% test data (about 643 records). This approach ensures that the model is trained on a large portion of the data while having sufficient separate subsets to tune the model parameters and evaluate its generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth = 0 :  0.7387247278382582\n",
      "max_depth = 1 :  0.7573872472783826\n",
      "max_depth = 2 :  0.7651632970451011\n",
      "max_depth = 3 :  0.7636080870917574\n",
      "max_depth = 3 :  0.7589424572317263\n",
      "\n",
      "Best Paramaters\n",
      "max_depth = 3 : 0.7651632970451011\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "best_accuracy = 0\n",
    "best_depth = 0\n",
    "for depth in range(1,6):\n",
    "    model = DecisionTreeClassifier(random_state=12345, max_depth=depth)\n",
    "    model.fit(features_train, target_train)\n",
    "    predictions_valid = model.predict(features_valid)\n",
    "    accuracy = accuracy_score(target_valid, predictions_valid)\n",
    "    print('max_depth =', best_depth, ': ', accuracy)\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_depth = depth\n",
    "        \n",
    "print()\n",
    "print('Best Paramaters')\n",
    "print('max_depth =', best_depth, ':', best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "The best performance for the Decision Tree model was achieved with a max_depth of 3, yielding a validation accuracy of 76.52%. Deeper trees resulted in slightly reduced accuracy, indicating that max_depth=3 provides the optimal balance between model complexity and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimators :  10 :  0.7713841368584758\n",
      "Estimators :  20 :  0.7713841368584758\n",
      "Estimators :  30 :  0.7698289269051322\n",
      "Estimators :  40 :  0.7713841368584758\n",
      "Estimators :  50 :  0.7667185069984448\n",
      "\n",
      "Best Parameters\n",
      "Estimators: 10 : 0.7713841368584758\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "best_accuracy = 0\n",
    "best_estimators = 0\n",
    "depth = 3\n",
    "\n",
    "for est in range(10, 51, 10):\n",
    "    model = RandomForestClassifier(random_state=12345, n_estimators=est, max_depth=depth)\n",
    "    model.fit(features_train, target_train)\n",
    "    predictions_valid = model.predict(features_valid)\n",
    "    accuracy = accuracy_score(target_valid, predictions_valid)\n",
    "    \n",
    "    print('Estimators : ', est, ': ', accuracy)\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_est = est\n",
    "        best_accuracy = accuracy\n",
    "        \n",
    "print()\n",
    "print('Best Parameters')\n",
    "print('Estimators:', best_est, ':', best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimators :  1 :  0.7573872472783826\n",
      "Estimators :  2 :  0.7573872472783826\n",
      "Estimators :  3 :  0.76049766718507\n",
      "Estimators :  4 :  0.76049766718507\n",
      "Estimators :  5 :  0.76049766718507\n",
      "Estimators :  6 :  0.7651632970451011\n",
      "Estimators :  7 :  0.7682737169517885\n",
      "Estimators :  8 :  0.7698289269051322\n",
      "Estimators :  9 :  0.7682737169517885\n",
      "Estimators :  10 :  0.7713841368584758\n",
      "Estimators :  11 :  0.7744945567651633\n",
      "Estimators :  12 :  0.7682737169517885\n",
      "Estimators :  13 :  0.7744945567651633\n",
      "Estimators :  14 :  0.7698289269051322\n",
      "Estimators :  15 :  0.7776049766718507\n",
      "Estimators :  16 :  0.7682737169517885\n",
      "Estimators :  17 :  0.7729393468118196\n",
      "Estimators :  18 :  0.7698289269051322\n",
      "Estimators :  19 :  0.7729393468118196\n",
      "Estimators :  20 :  0.7713841368584758\n",
      "\n",
      "Best Parameters\n",
      "Estimators: 15 : 0.7776049766718507\n"
     ]
    }
   ],
   "source": [
    "# Random Forest 2\n",
    "best_accuracy = 0\n",
    "best_estimators = 0\n",
    "depth = 3\n",
    "\n",
    "for est in range(1, 21):\n",
    "    model = RandomForestClassifier(random_state=12345, n_estimators=est, max_depth=depth)\n",
    "    model.fit(features_train, target_train)\n",
    "    predictions_valid = model.predict(features_valid)\n",
    "    accuracy = accuracy_score(target_valid, predictions_valid)\n",
    "    \n",
    "    print('Estimators : ', est, ': ', accuracy)\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_est = est\n",
    "        best_accuracy = accuracy\n",
    "        \n",
    "print()\n",
    "print('Best Parameters')\n",
    "print('Estimators:', best_est, ':', best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "Two Random Forest experiments with a fixed max_depth of 3, we first evaluated models with n_estimators in coarse increments, and then refined the search with values from 1 to 20. The initial run showed that 10 estimators yielded a validation accuracy of approximately 77.14%. Further investigation revealed that as we increased the number of trees, the accuracy steadily improved, peaking at 15 estimators with an accuracy of around 77.76%. Beyond this point, accuracy began to fluctuate slightly. This suggests that, for our dataset and with a max_depth of 3, a Random Forest model configured with 15 estimators achieves the optimal balance between model complexity and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7293934681181959\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "model = LinearRegression()\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "predictions_valid = model.predict(features_valid)\n",
    "predictions_binary = (predictions_valid > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(target_valid, predictions_binary)\n",
    "\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "Using Linear Regression, the model was trained on the training data and then generated continuous predictions for the validation set. By applying a threshold of 0.5, these continuous predictions were converted into binary classes. This approach resulted in an accuracy of approximately 72.94% on the validation set. While this performance provides a useful baseline, it falls short of the 75% accuracy target, indicating that more specialized classification methods might be better suited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "Best Model = Random Forest with 15 estimators at depth of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7838258164852255\n"
     ]
    }
   ],
   "source": [
    "# Test Model\n",
    "best_model = RandomForestClassifier(random_state=12345, n_estimators=15, max_depth=3)\n",
    "best_model.fit(features_train, target_train)\n",
    "\n",
    "predictions_test = best_model.predict(features_test)\n",
    "\n",
    "accuracy_test = accuracy_score(target_test, predictions_test)\n",
    "\n",
    "print('Test Accuracy:', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "The best-performing Random Forest model, configured with 15 estimators and a maximum depth of 3, achieved a test accuracy of approximately 78.38%. This result exceeds the 75% target threshold and demonstrates the model's robustness and generalization ability when applied to unseen data. The high accuracy on the test set confirms that the chosen hyperparameters effectively balance model complexity and predictive performance, making it a suitable candidate for recommending the appropriate subscriber plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Test:\n",
    "This model passes sanity test by having an accuracy score above 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This analysis demonstrates that by leveraging a high-quality, complete dataset of 3214 records capturing voice and data usage, we effectively developed and evaluated multiple models to recommend the optimal plan—Smart or Ultra—for Megaline subscribers. The data was strategically split into 60% training, 20% validation, and 20% test sets, ensuring robust model development and assessment. Among the models tested, the Decision Tree achieved its best validation performance at a max_depth of 3 with an accuracy of about 76.52%, while the Linear Regression approach, after thresholding its continuous predictions, reached only around 72.94%, falling short of the desired 75% threshold. In contrast, the Random Forest model, fine-tuned with a fixed max_depth of 3 and 15 estimators, not only achieved a validation accuracy of approximately 77.76% but also delivered a test accuracy of roughly 78.38%. This robust performance confirms that the Random Forest configuration effectively balances model complexity and generalization, making it a highly reliable candidate for accurately transitioning subscribers to the appropriate new plan."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 835,
    "start_time": "2025-02-10T14:17:19.960Z"
   },
   {
    "duration": 20,
    "start_time": "2025-02-10T14:18:18.736Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-10T14:18:24.494Z"
   },
   {
    "duration": 870,
    "start_time": "2025-02-10T14:23:47.640Z"
   },
   {
    "duration": 10,
    "start_time": "2025-02-10T14:24:22.805Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-10T14:24:31.851Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-10T14:26:20.306Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-10T14:27:09.172Z"
   },
   {
    "duration": 17,
    "start_time": "2025-02-10T14:29:25.092Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-10T14:30:23.472Z"
   },
   {
    "duration": 8,
    "start_time": "2025-02-10T14:38:31.777Z"
   },
   {
    "duration": 22,
    "start_time": "2025-02-10T14:39:11.340Z"
   },
   {
    "duration": 7,
    "start_time": "2025-02-10T14:39:37.972Z"
   },
   {
    "duration": 24,
    "start_time": "2025-02-10T14:46:53.715Z"
   },
   {
    "duration": 24,
    "start_time": "2025-02-10T14:51:37.118Z"
   },
   {
    "duration": 26,
    "start_time": "2025-02-10T14:52:00.664Z"
   },
   {
    "duration": 26,
    "start_time": "2025-02-10T14:52:38.613Z"
   },
   {
    "duration": 25,
    "start_time": "2025-02-10T14:53:01.808Z"
   },
   {
    "duration": 24,
    "start_time": "2025-02-10T14:53:18.384Z"
   },
   {
    "duration": 25,
    "start_time": "2025-02-10T14:53:38.022Z"
   },
   {
    "duration": 24,
    "start_time": "2025-02-10T14:54:31.966Z"
   },
   {
    "duration": 26,
    "start_time": "2025-02-10T14:55:03.222Z"
   },
   {
    "duration": 26,
    "start_time": "2025-02-10T14:55:51.642Z"
   },
   {
    "duration": 25,
    "start_time": "2025-02-10T14:56:07.371Z"
   },
   {
    "duration": 224,
    "start_time": "2025-02-10T15:07:06.968Z"
   },
   {
    "duration": 180,
    "start_time": "2025-02-10T15:08:14.301Z"
   },
   {
    "duration": 350,
    "start_time": "2025-02-10T15:08:27.209Z"
   },
   {
    "duration": 726,
    "start_time": "2025-02-10T15:13:03.930Z"
   },
   {
    "duration": 17,
    "start_time": "2025-02-10T15:13:04.659Z"
   },
   {
    "duration": 10,
    "start_time": "2025-02-10T15:13:04.678Z"
   },
   {
    "duration": 26,
    "start_time": "2025-02-10T15:13:04.690Z"
   },
   {
    "duration": 253,
    "start_time": "2025-02-10T15:13:04.718Z"
   },
   {
    "duration": 370,
    "start_time": "2025-02-10T15:13:04.973Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-10T15:13:05.344Z"
   },
   {
    "duration": 285,
    "start_time": "2025-02-10T15:15:04.635Z"
   },
   {
    "duration": 113,
    "start_time": "2025-02-10T15:15:13.325Z"
   },
   {
    "duration": 810,
    "start_time": "2025-02-10T15:16:02.548Z"
   },
   {
    "duration": 13,
    "start_time": "2025-02-10T15:16:03.361Z"
   },
   {
    "duration": 8,
    "start_time": "2025-02-10T15:16:03.375Z"
   },
   {
    "duration": 25,
    "start_time": "2025-02-10T15:16:03.384Z"
   },
   {
    "duration": 249,
    "start_time": "2025-02-10T15:16:03.411Z"
   },
   {
    "duration": 360,
    "start_time": "2025-02-10T15:16:03.662Z"
   },
   {
    "duration": 319,
    "start_time": "2025-02-10T15:16:04.024Z"
   },
   {
    "duration": 7,
    "start_time": "2025-02-10T15:18:24.011Z"
   },
   {
    "duration": 29,
    "start_time": "2025-02-10T15:24:02.883Z"
   },
   {
    "duration": 777,
    "start_time": "2025-02-10T15:31:31.386Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-10T15:31:32.166Z"
   },
   {
    "duration": 8,
    "start_time": "2025-02-10T15:31:32.181Z"
   },
   {
    "duration": 31,
    "start_time": "2025-02-10T15:31:32.191Z"
   },
   {
    "duration": 235,
    "start_time": "2025-02-10T15:31:32.242Z"
   },
   {
    "duration": 368,
    "start_time": "2025-02-10T15:31:32.479Z"
   },
   {
    "duration": 7,
    "start_time": "2025-02-10T15:31:32.848Z"
   },
   {
    "duration": 29,
    "start_time": "2025-02-10T15:31:32.856Z"
   },
   {
    "duration": 8,
    "start_time": "2025-02-10T16:28:41.709Z"
   },
   {
    "duration": 828,
    "start_time": "2025-02-10T16:31:23.361Z"
   },
   {
    "duration": 16,
    "start_time": "2025-02-10T16:31:24.191Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-10T16:31:24.210Z"
   },
   {
    "duration": 27,
    "start_time": "2025-02-10T16:31:24.221Z"
   },
   {
    "duration": 264,
    "start_time": "2025-02-10T16:31:24.251Z"
   },
   {
    "duration": 393,
    "start_time": "2025-02-10T16:31:24.517Z"
   },
   {
    "duration": 8,
    "start_time": "2025-02-10T16:31:24.912Z"
   },
   {
    "duration": 30,
    "start_time": "2025-02-10T16:31:24.922Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
